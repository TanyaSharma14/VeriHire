import argparse
import json
import os
from datetime import datetime

import numpy as np
import pandas as pd
from sentence_transformers import SentenceTransformer
from sklearn.metrics import (
    precision_recall_fscore_support,
    confusion_matrix,
    classification_report,
)

def safe_text(x):
    if x is None:
        return ""
    if not isinstance(x, str):
        x = str(x)
    return x.strip()

def compute_scores(model, resumes, jobs, batch_size=32):
    """
    Compute cosine similarity scores in [0, 100] for resume-job pairs.
    """
    # Encode in batches for speed
    res_emb = model.encode(resumes, batch_size=batch_size, show_progress_bar=True)
    job_emb = model.encode(jobs, batch_size=batch_size, show_progress_bar=True)

    # Cosine similarity for corresponding rows
    res_norm = res_emb / np.linalg.norm(res_emb, axis=1, keepdims=True)
    job_norm = job_emb / np.linalg.norm(job_emb, axis=1, keepdims=True)
    sims = np.sum(res_norm * job_norm, axis=1)  # [-1, 1]

    scores = np.clip(sims * 100.0, -100.0, 100.0)
    return scores

def evaluate_at_threshold(y_true, scores, threshold):
    y_pred = (scores >= threshold).astype(int)

    prec, rec, f1, _ = precision_recall_fscore_support(
        y_true, y_pred, average="binary", zero_division=0
    )

    cm = confusion_matrix(y_true, y_pred).tolist()  # [[tn, fp], [fn, tp]]
    return {
        "threshold": float(threshold),
        "precision": float(prec),
        "recall": float(rec),
        "f1": float(f1),
        "confusion_matrix": cm,
        "y_pred": y_pred,
    }

def sweep_thresholds(y_true, scores, start=0.0, end=100.0, step=1.0):
    best = None
    t = start
    while t <= end:
        res = evaluate_at_threshold(y_true, scores, t)
        if best is None or res["f1"] > best["f1"]:
            best = res
        t += step
    return best

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", required=True, help="Path to CSV with resume_text, job_text, label")
    parser.add_argument("--model", default="all-MiniLM-L6-v2", help="SentenceTransformer model name/path")
    parser.add_argument("--threshold", type=float, default=60.0, help="Threshold for match classification (0-100)")
    parser.add_argument("--sweep", action="store_true", help="Sweep thresholds to find best F1")
    parser.add_argument("--step", type=float, default=1.0, help="Step size for threshold sweep")
    parser.add_argument("--batch", type=int, default=32, help="Embedding batch size")
    args = parser.parse_args()

    df = pd.read_csv(args.data)

    required = {"resume_text", "job_text", "label"}
    missing = required - set(df.columns)
    if missing:
        raise ValueError(f"Missing columns in CSV: {sorted(list(missing))}")

    df["resume_text"] = df["resume_text"].apply(safe_text)
    df["job_text"] = df["job_text"].apply(safe_text)

    # Basic clean: drop empty rows
    df = df[(df["resume_text"] != "") & (df["job_text"] != "")]
    if len(df) == 0:
        raise ValueError("No valid rows after cleaning (empty resume/job text).")

    # Labels
    y_true = df["label"].astype(int).to_numpy()

    print(f"\nLoaded {len(df)} labeled pairs from: {args.data}")
    print(f"Label distribution: {dict(pd.Series(y_true).value_counts())}")

    print(f"\nLoading embedding model: {args.model}")
    model = SentenceTransformer(args.model)

    print("\nComputing similarity scores...")
    scores = compute_scores(model, df["resume_text"].tolist(), df["job_text"].tolist(), batch_size=args.batch)

    # Choose threshold
    if args.sweep:
        print("\nSweeping thresholds for best F1...")
        best = sweep_thresholds(y_true, scores, start=0.0, end=100.0, step=args.step)
        threshold = best["threshold"]
        metrics = best
        print(f"Best threshold: {threshold:.2f}")
    else:
        threshold = args.threshold
        metrics = evaluate_at_threshold(y_true, scores, threshold)

    # Pretty output
    y_pred = (scores >= threshold).astype(int)
    print("\n=== Evaluation Results ===")
    print(f"Threshold: {threshold:.2f}")
    print(f"Precision: {metrics['precision']:.4f}")
    print(f"Recall:    {metrics['recall']:.4f}")
    print(f"F1-score:  {metrics['f1']:.4f}")
    print("Confusion Matrix [ [TN, FP], [FN, TP] ]:")
    print(metrics["confusion_matrix"])

    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, zero_division=0))

    # Save results
    out_dir = os.path.join(os.path.dirname(__file__), "results")
    os.makedirs(out_dir, exist_ok=True)
    stamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_path = os.path.join(out_dir, f"eval_{stamp}.json")

    payload = {
        "data_path": args.data,
        "model": args.model,
        "threshold": float(threshold),
        "precision": metrics["precision"],
        "recall": metrics["recall"],
        "f1": metrics["f1"],
        "confusion_matrix": metrics["confusion_matrix"],
        "n_samples": int(len(df)),
        "label_distribution": dict(pd.Series(y_true).value_counts().to_dict()),
        "notes": "Binary match evaluation using similarity threshold over sentence-transformer embeddings."
    }

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(payload, f, indent=2)

    print(f"\nSaved report to: {out_path}")

if __name__ == "__main__":
    main()

